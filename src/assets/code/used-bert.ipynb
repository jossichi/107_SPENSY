{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertTokenizer\nfrom transformers import MarianMTModel, MarianTokenizer\nimport time\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom torch.utils.data import Dataset\nfrom transformers import EarlyStoppingCallback\n\nmodel_name = \"Helsinki-NLP/opus-mt-vi-en\"\ntokenizer_translate = MarianTokenizer.from_pretrained(model_name)\nmodel_translate = MarianMTModel.from_pretrained(model_name)\n\n# Load model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"/kaggle/input/107-spensy-ai/pytorch/default/1\")\ntokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/107-spensy-ai/pytorch/default/1\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-05T03:23:47.704081Z","iopub.execute_input":"2024-11-05T03:23:47.704600Z","iopub.status.idle":"2024-11-05T03:24:09.597076Z","shell.execute_reply.started":"2024-11-05T03:23:47.704554Z","shell.execute_reply":"2024-11-05T03:24:09.595556Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Container for misclassified examples\nmisclassified_data = []\n\n# Define dataset class\nclass SentimentDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T03:24:09.599832Z","iopub.execute_input":"2024-11-05T03:24:09.601261Z","iopub.status.idle":"2024-11-05T03:24:09.612848Z","shell.execute_reply.started":"2024-11-05T03:24:09.601194Z","shell.execute_reply":"2024-11-05T03:24:09.610794Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def translate_vi_to_en(text):\n    # Làm sạch và chuẩn hóa văn bản nếu cần\n    text = text.strip()\n    inputs = tokenizer_translate(text, return_tensors=\"pt\", truncation=True)\n    translated = model_translate.generate(**inputs)\n    translated_text = tokenizer_translate.decode(translated[0], skip_special_tokens=True)\n    return translated_text","metadata":{"execution":{"iopub.status.busy":"2024-11-05T03:24:09.614599Z","iopub.execute_input":"2024-11-05T03:24:09.615127Z","iopub.status.idle":"2024-11-05T03:24:09.626338Z","shell.execute_reply.started":"2024-11-05T03:24:09.615080Z","shell.execute_reply":"2024-11-05T03:24:09.624291Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef classify_text(translated_text):\n    inputs = tokenizer(translated_text, return_tensors=\"pt\", truncation=True, padding=True)\n    outputs = model(**inputs)\n    prediction = torch.argmax(outputs.logits, dim=1).item()\n    return \"Positive\" if prediction == 1 else \"Negative\"","metadata":{"execution":{"iopub.status.busy":"2024-11-05T03:30:31.257687Z","iopub.execute_input":"2024-11-05T03:30:31.258208Z","iopub.status.idle":"2024-11-05T03:30:31.266163Z","shell.execute_reply.started":"2024-11-05T03:30:31.258162Z","shell.execute_reply":"2024-11-05T03:30:31.264490Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"new_text = input(\"Nhập câu bình luận vào đây: \")\n\n# Translation timing\nstart_translate = time.time()\ntranslated_text = translate_vi_to_en(new_text)\nend_translate = time.time()\nprint(\"Translated Text:\", translated_text)\nprint(\"Translation Time:\", end_translate - start_translate, \"seconds\")\n\n# Classification timing\nstart_classify = time.time()\nprediction = classify_text(translated_text)\nend_classify = time.time()\nprint(\"Kết quả dự đoán:\", prediction)\nprint(\"Thời gian thực thi:\", end_classify - start_classify, \"seconds\")\n\n# Accuracy feedback\ntrue_label = input(\"Cho tôi giá trị thực sự (Positive/Negative): \").strip().capitalize()\nif true_label in [\"Positive\", \"Negative\"]:\n    accuracy = accuracy_score([true_label], [prediction])\n    print(\"Độ chính xác:\", accuracy)\n    if accuracy == 0:\n        misclassified_data.append({'text': translated_text, 'label': 1 if true_label == \"Positive\" else 0})\n        print(\"Cảm ơn bạn đã feedback chúng tôi sẽ huấn luyện lại mô hình cho tốt hơn.\")\nelse:\n    print(\"Invalid label input.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T03:44:08.833363Z","iopub.execute_input":"2024-11-05T03:44:08.834725Z","iopub.status.idle":"2024-11-05T03:44:52.448721Z","shell.execute_reply.started":"2024-11-05T03:44:08.834675Z","shell.execute_reply":"2024-11-05T03:44:52.447249Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdin","text":"Nhập câu bình luận vào đây:  Thiệt là nhiều khi muốn làm Taowork cho khỏe Kêu làm nội dung thì tìm trên gg xong copy y chang lại rồi gửi, không thêm bớt, không sửa font hay chính tả gì cả Kêu làm word với ppt thì làm cho có, ko căng chỉnh font, lề gì. Ppt thì để 1 đống chữ ko biết tự lược lại nội dung cho ngắn gọn, sai chính tả lum la, kêu chỉnh cả trăm lần cuối cùng đứa đi nhắc phải là đứa tự chỉnh Kêu thuyết thì thì ngại, ko muốn. Có lên thì ppt có gì đọc y chang, không xem trước bài ở nhà. Đọc cho có rồi về Nhiều khi muốn xin giảng viên làm bài 1 mình cho rồi nhưng gv ko cho chứ làm việc nhóm mà gặp đủ 3 thể loại như trên, t mà là nhóm trưởng t trừ điểm hết khỏi ý kiến với t\n"},{"name":"stdout","text":"Translated Text: A lot of times you want to make Taowork healthy. If you want to do the content, you can do the same copy on your gg and then you can send, no less, no more font or spelling. If you want to do it with ppt, then you can do it.\nTranslation Time: 4.5567896366119385 seconds\nKết quả dự đoán: Positive\nThời gian thực thi: 0.1603682041168213 seconds\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Cho tôi giá trị thực sự (Positive/Negative):  negative\n"},{"name":"stdout","text":"Độ chính xác: 0.0\nCảm ơn bạn đã feedback chúng tôi sẽ huấn luyện lại mô hình cho tốt hơn.\n","output_type":"stream"}]},{"cell_type":"code","source":"def classify_and_get_feedback(text):\n    translated_text = translate_vi_to_en(text)\n    print(\"Translated Text:\", translated_text)\n\n    start_classify = time.time()\n    predicted_label = classify_text(translated_text)\n    end_classify = time.time()\n    print(\"Predicted Label:\", predicted_label)\n    print(\"Classification Time:\", end_classify - start_classify, \"seconds\")\n\n    user_label = input(\"If incorrect, please enter the correct label (Positive/Negative), else press Enter: \")\n    if user_label and user_label != predicted_label:\n        misclassified_data.append({'text': translated_text, 'label': 1 if user_label == \"Positive\" else 0})\n        print(\"Feedback recorded. Model will learn from this.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T03:26:51.517347Z","iopub.execute_input":"2024-11-05T03:26:51.517850Z","iopub.status.idle":"2024-11-05T03:26:51.527784Z","shell.execute_reply.started":"2024-11-05T03:26:51.517794Z","shell.execute_reply":"2024-11-05T03:26:51.525888Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def retrain_with_feedback(train_dataset):\n    if misclassified_data:\n        feedback_df = pd.DataFrame(misclassified_data)\n        feedback_encodings = tokenizer(feedback_df['text'].tolist(), truncation=True, padding=True)\n        feedback_labels = feedback_df['label'].tolist()\n\n        feedback_dataset = SentimentDataset(feedback_encodings, feedback_labels)\n        combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset, feedback_dataset])\n\n        training_args = TrainingArguments(\n            output_dir='./results',\n            num_train_epochs=1,  \n            per_device_train_batch_size=8,\n            evaluation_strategy=\"no\",\n            save_steps=500,\n            logging_dir='./logs',\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=combined_train_dataset,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n\n        # Retrain model\n        trainer.train()\n        print(\"Model retrained with user feedback.\")\n        misclassified_data.clear()  \n\nwhile True:\n    new_text = input(\"Enter a sentence for classification (or 'quit' to exit): \")\n    if new_text.lower() == 'quit':\n        break\n    classify_and_get_feedback(new_text)\n\n    # Huấn luyện lại nếu dữ liệu >=5 \n    if len(misclassified_data) >= 5:  # Adjust threshold as needed\n        retrain_with_feedback(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T03:41:32.737699Z","iopub.execute_input":"2024-11-05T03:41:32.738648Z","iopub.status.idle":"2024-11-05T03:42:23.820081Z","shell.execute_reply.started":"2024-11-05T03:41:32.738590Z","shell.execute_reply":"2024-11-05T03:42:23.818651Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a sentence for classification (or 'quit' to exit):  Thiệt là nhiều khi muốn làm Taowork cho khỏe Kêu làm nội dung thì tìm trên gg xong copy y chang lại rồi gửi, không thêm bớt, không sửa font hay chính tả gì cả Kêu làm word với ppt thì làm cho có, ko căng chỉnh font, lề gì. Ppt thì để 1 đống chữ ko biết tự lược lại nội dung cho ngắn gọn, sai chính tả lum la, kêu chỉnh cả trăm lần cuối cùng đứa đi nhắc phải là đứa tự chỉnh Kêu thuyết thì thì ngại, ko muốn. Có lên thì ppt có gì đọc y chang, không xem trước bài ở nhà. Đọc cho có rồi về Nhiều khi muốn xin giảng viên làm bài 1 mình cho rồi nhưng gv ko cho chứ làm việc nhóm mà gặp đủ 3 thể loại như trên, t mà là nhóm trưởng t trừ điểm hết khỏi ý kiến với t\n"},{"name":"stdout","text":"Translated Text: A lot of times you want to make Taowork healthy. If you want to do the content, you can do the same copy on your gg and then you can send, no less, no more font or spelling. If you want to do it with ppt, then you can do it.\nPredicted Label: Positive\nClassification Time: 0.15396928787231445 seconds\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"If incorrect, please enter the correct label (Positive/Negative), else press Enter:  negative\n"},{"name":"stdout","text":"Feedback recorded. Model will learn from this.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a sentence for classification (or 'quit' to exit):  quit\n"}]}]}